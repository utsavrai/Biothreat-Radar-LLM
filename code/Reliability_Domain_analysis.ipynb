{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GcyBiMux3LxR1VKY72PArR74",
      "metadata": {
        "id": "GcyBiMux3LxR1VKY72PArR74",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def fetch_citywise_news(api_key, city, keyword, from_date, to_date):\n",
        "    url = \"https://v3-api.newscatcherapi.com/api/search\"\n",
        "    headers = {'x-api-token': api_key, 'User-agent': 'your bot 0.1'}\n",
        "    params = {\n",
        "        'q': f\"{keyword} AND {city}\",\n",
        "        'from_': from_date,\n",
        "        'to_': to_date,\n",
        "        'countries': 'GB',\n",
        "        'lang': ['en'],\n",
        "        'page_size': 50\n",
        "    }\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    response.raise_for_status()  # Check for HTTP errors\n",
        "    if response.status_code == 429:\n",
        "        time.sleep(int(response.headers[\"Retry-After\"]))\n",
        "    return response.json()\n",
        "\n",
        "def extract_domain(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    # Return scheme and netloc which constitutes the base domain\n",
        "    return f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "\n",
        "def fetch_and_process_news():\n",
        "    api_key = 'yourkeyhere'\n",
        "    cities = [\"London\", \"Edinburgh\", \"Cardiff\", \"Belfast\", \"Birmingham\", \"Manchester\",\n",
        "              \"Liverpool\", \"Bristol\", \"Glasgow\", \"Sheffield\", \"Leeds\", \"Newcastle\",\n",
        "              \"Nottingham\", \"Leicester\"]\n",
        "    keyword = 'disease outbreak'\n",
        "    from_date = '2023-10-01'\n",
        "    to_date = '2024-04-01'\n",
        "\n",
        "    all_domains = []\n",
        "\n",
        "    for city in cities:\n",
        "        news_data = fetch_citywise_news(api_key, city, keyword, from_date, to_date)\n",
        "        if news_data.get('status') == 'ok':\n",
        "            for article in news_data['articles']:\n",
        "                if 'all_links' in article and article['all_links']:\n",
        "                    for link in article['all_links']:\n",
        "                        domain = extract_domain(link)\n",
        "                        all_domains.append(domain)\n",
        "\n",
        "    # Count the occurrences of each domain and select the top 10 most frequent\n",
        "    domain_counts = Counter(all_domains)\n",
        "    top_domains = domain_counts.most_common(10)\n",
        "    return [{\"Domain\": domain, \"Count\": count} for domain, count in top_domains]\n",
        "\n",
        "def save_domains_to_csv(domain_links):\n",
        "    df = pd.DataFrame(domain_links)\n",
        "    df.to_csv('top_10_frequent_domains.csv', index=False)\n",
        "\n",
        "def main():\n",
        "    domain_links = fetch_and_process_news()\n",
        "    save_domains_to_csv(domain_links)\n",
        "    print(\"Saved the top 10 most frequent domains to top_10_frequent_domains.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "utsav.rai (Apr 17, 2024, 10:02:11â€¯AM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
