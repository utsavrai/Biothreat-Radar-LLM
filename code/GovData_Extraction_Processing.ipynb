{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88C86PAfHQ1A",
        "outputId": "0d1c85f3-d02e-4bba-b4aa-d4679a6c3d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "def download_csv_files(url, download_folder):\n",
        "    # Send HTTP request to the URL\n",
        "    response = requests.get(url)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        print('Failed to retrieve webpage')\n",
        "        return\n",
        "\n",
        "    # Use BeautifulSoup to parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all anchor tags <a> and filter by those ending with '.csv'\n",
        "    csv_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.csv')]\n",
        "\n",
        "    # Ensure the download folder exists\n",
        "    os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "    # Download each CSV file found\n",
        "    for link in csv_links:\n",
        "        # Construct the full URL if the link is relative\n",
        "        if not link.startswith('http'):\n",
        "            link = url + link\n",
        "\n",
        "        # Extract the filename from the URL\n",
        "        filename = os.path.join(download_folder, link.split('/')[-1])\n",
        "\n",
        "        # Send request to download the CSV file\n",
        "        file_response = requests.get(link)\n",
        "        if file_response.status_code == 200:\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(file_response.content)\n",
        "            print(f'Downloaded {filename}')\n",
        "        else:\n",
        "            print(f'Failed to download {link}')\n",
        "\n",
        "# Example usage\n",
        "url = 'https://www.data.gov.uk/dataset/e37520b0-ddb4-4cfa-b53f-a9c50ef21965/notification-of-infectious-diseases'  # Replace with the actual URL\n",
        "download_folder = 'downloaded_csvs'  # Folder where files will be saved\n",
        "download_csv_files(url, download_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afbZmiCAHXoM",
        "outputId": "23e33b4f-fd59-4173-a17f-efbfcefb47b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded downloaded_csvs/noids-week-1-4-2024-.csv\n",
            "Downloaded downloaded_csvs/noids-week-52-csv.csv\n",
            "Downloaded downloaded_csvs/noids-week-49-csv.csv\n",
            "Downloaded downloaded_csvs/noids-week-45-csv.csv\n",
            "Downloaded downloaded_csvs/noids-week-41-csv.csv\n",
            "Downloaded downloaded_csvs/noids-week-37-csv.csv\n",
            "Downloaded downloaded_csvs/noids-week-32-csv-.csv\n",
            "Downloaded downloaded_csvs/noids-week-33-csv-.csv\n",
            "Downloaded downloaded_csvs/noids-week-28-csv.csv\n",
            "Downloaded downloaded_csvs/noids-week-24-csv.csv\n",
            "Downloaded downloaded_csvs/noids-report-2023-week-17-20.csv\n",
            "Downloaded downloaded_csvs/noids-report-2023-week-13-16.csv\n",
            "Downloaded downloaded_csvs/copy-of-noids-week-52-csv.csv\n",
            "Downloaded downloaded_csvs/copy-of-noids-week-48-csv.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-44-2022-csv.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-37--40-2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-33---36-2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-33---36-2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-week--29---32--2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-25---28--2022.csv\n",
            "Downloaded downloaded_csvs/noids-report--week-21---24.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-17---20--2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-13---16--2022.csv\n",
            "Downloaded downloaded_csvs/noids--report-week--9---12--2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-week--5---8-2022.csv\n",
            "Downloaded downloaded_csvs/noids--report-week-1---4--2022.csv\n",
            "Downloaded downloaded_csvs/noids-report-2021-week-52.csv\n",
            "Downloaded downloaded_csvs/noids--report-2021-weeks-48--51.csv\n",
            "Downloaded downloaded_csvs/noids-report-2021-weeks-44-47.csv\n",
            "Downloaded downloaded_csvs/noids-report-2021-week-40-43.csv\n",
            "Downloaded downloaded_csvs/noids-week-36-39-2021.csv\n",
            "Downloaded downloaded_csvs/noids-report-2021-weeks-32-35.csv\n",
            "Downloaded downloaded_csvs/noids-week-7-10-2021-updated.csv\n",
            "Downloaded downloaded_csvs/noids-week-30-and-31-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-27---29-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-23--26-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-19---22-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-15-18-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-11--14-2021-.csv\n",
            "Downloaded downloaded_csvs/noids-week-4-6-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-1-3-2021.csv\n",
            "Downloaded downloaded_csvs/noids-week-51-to-week-53-2020.csv\n",
            "Downloaded downloaded_csvs/noids-week-51-to-week-53-2020.csv\n",
            "Downloaded downloaded_csvs/noids-week-47-to-week-50-2020.csv\n",
            "Downloaded downloaded_csvs/noids-week-44-to-week-46-2020.csv\n",
            "Downloaded downloaded_csvs/noids-week-40---week-43.csv\n",
            "Downloaded downloaded_csvs/noids-week-40---week-43.csv\n",
            "Downloaded downloaded_csvs/noids-week-36-to-week-39.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-53.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-40.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-39.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-38.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-37.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-36.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-35.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-34.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-33.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-32.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-31.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-30.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-29.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-28.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-27.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-26.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-25.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-24.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-23.csv\n",
            "Downloaded downloaded_csvs/noidss-report-2015-week-21.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-22.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-20-2015.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-week-19-2015.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-week-18-2015.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-17-2015.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-week-16-2015.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-week-15-2015.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-2015-week-14.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-13-2015.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-2015-week-12.csv\n",
            "Downloaded downloaded_csvs/weekly-noids-report-11.csv\n",
            "Downloaded downloaded_csvs/noids-report-week-10-2015.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-9.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-8.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-7.csv\n",
            "Downloaded downloaded_csvs/noids-report-2015-week-6.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!zip -r /content/downloaded_csv.zip /content/downloaded_csvs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQIv5tKWHqZJ",
        "outputId": "7d24d09a-54c2-4e16-a83f-12bc99d5b135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/downloaded_csvs/ (stored 0%)\n",
            "  adding: content/downloaded_csvs/noids-report--week-21---24.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-week-19-2015.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-37--40-2022.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2021-weeks-44-47.csv (deflated 51%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-30.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-40---week-43.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-week-28-csv.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-week-41-csv.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2023-week-13-16.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-week-45-csv.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-28.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-24-csv.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-39.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids--report-2021-weeks-48--51.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-38.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-20-2015.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-week-30-and-31-2021.csv (deflated 49%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-11.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-26.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids--report-week-1---4--2022.csv (deflated 56%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-8.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-week-15-2015.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-34.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-week-16-2015.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-9.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-7.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2021-week-40-43.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-week-51-to-week-53-2020.csv (deflated 50%)\n",
            "  adding: content/downloaded_csvs/noids-week-11--14-2021-.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-40.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-53.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-15-18-2021.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-24.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-week--5---8-2022.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-week-36-39-2021.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-23.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2021-weeks-32-35.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-week-18-2015.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noidss-report-2015-week-21.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-6.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2023-week-17-20.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-week-1-3-2021.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-week-7-10-2021-updated.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-week-4-6-2021.csv (deflated 100%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-32.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-35.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-27.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-13---16--2022.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-report-2021-week-52.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-25.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-13-2015.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-22.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-29.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-52-csv.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-17-2015.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-week-32-csv-.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-week-37-csv.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-week-1-4-2024-.csv (deflated 56%)\n",
            "  adding: content/downloaded_csvs/noids-week-47-to-week-50-2020.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/copy-of-noids-week-52-csv.csv (deflated 97%)\n",
            "  adding: content/downloaded_csvs/noids-week-33-csv-.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/copy-of-noids-week-48-csv.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-36.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-49-csv.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-week-44-to-week-46-2020.csv (deflated 50%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-17---20--2022.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-2015-week-14.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-19---22-2021.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-10-2015.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-report-week--29---32--2022.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-37.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids--report-week--9---12--2022.csv (deflated 54%)\n",
            "  adding: content/downloaded_csvs/noids-week-36-to-week-39.csv (deflated 52%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-33.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/weekly-noids-report-2015-week-12.csv (deflated 61%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-33---36-2022.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-25---28--2022.csv (deflated 55%)\n",
            "  adding: content/downloaded_csvs/noids-week-23--26-2021.csv (deflated 53%)\n",
            "  adding: content/downloaded_csvs/noids-report-2015-week-31.csv (deflated 60%)\n",
            "  adding: content/downloaded_csvs/noids-week-27---29-2021.csv (deflated 51%)\n",
            "  adding: content/downloaded_csvs/noids-report-week-44-2022-csv.csv (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/downloaded_csv.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d2WCpRV6KO5g",
        "outputId": "96a60b7c-92a7-4fb0-af8d-3d8637fa93e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b30db3e1-f7f6-406c-9e9b-cfcd702485ee\", \"downloaded_csv.zip\", 86132)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas matplotlib reportlab PyPDF2"
      ],
      "metadata": {
        "id": "j_pc85UKKZ19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9049cf7d-0f4a-4b78-ad55-4c5227488242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, PageBreak\n",
        "from reportlab.lib import colors\n",
        "from PyPDF2 import PdfMerger\n",
        "\n",
        "def csv_to_pdf(csv_file, pdf_file):\n",
        "    # Load CSV data into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Create a PDF file using ReportLab\n",
        "    pdf = SimpleDocTemplate(pdf_file, pagesize=letter)\n",
        "    elements = []\n",
        "\n",
        "    # Define the table style\n",
        "    style = TableStyle([\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.lightblue),\n",
        "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "        ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
        "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
        "        ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
        "    ])\n",
        "\n",
        "    # Process the DataFrame in chunks to add to the PDF\n",
        "    chunk_size = 25  # Adjust chunk size based on your needs\n",
        "    for start_row in range(0, len(df), chunk_size):\n",
        "        end_row = min(start_row + chunk_size, len(df))\n",
        "        data = [df.columns.to_list()] + df.iloc[start_row:end_row].values.tolist()\n",
        "        t = Table(data)\n",
        "        t.setStyle(style)\n",
        "        elements.append(t)\n",
        "        elements.append(PageBreak())\n",
        "\n",
        "    # Remove the last PageBreak\n",
        "    if elements:\n",
        "        elements.pop()\n",
        "\n",
        "    # Build the PDF\n",
        "    pdf.build(elements)\n",
        "\n",
        "def merge_pdfs(pdf_files, output_path):\n",
        "    merger = PdfMerger()\n",
        "\n",
        "    for pdf in pdf_files:\n",
        "        merger.append(pdf)\n",
        "\n",
        "    merger.write(output_path)\n",
        "    merger.close()\n",
        "\n",
        "# Directory setup\n",
        "csv_directory = '/content/csvs'  # Update this path\n",
        "pdf_directory = '/content/pdfs'  # Update this path\n",
        "final_pdf_path = os.path.join(pdf_directory, 'merged_output.pdf')\n",
        "\n",
        "os.makedirs(pdf_directory, exist_ok=True)\n",
        "\n",
        "pdf_files = []\n",
        "\n",
        "# Convert each CSV to a PDF with a table\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        csv_path = os.path.join(csv_directory, filename)\n",
        "        pdf_path = os.path.join(pdf_directory, filename.replace('.csv', '.pdf'))\n",
        "        csv_to_pdf(csv_path, pdf_path)\n",
        "        pdf_files.append(pdf_path)\n",
        "        print(f\"PDF generated for {filename}\")\n",
        "\n",
        "# Merge all PDFs into one\n",
        "merge_pdfs(pdf_files, final_pdf_path)\n",
        "print(f\"All PDFs have been merged into: {final_pdf_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaoCgRpD8kwL",
        "outputId": "dc488ac0-8107-4056-e3f1-3af63c34fda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF generated for noids-week-4-6-2021.csv\n",
            "PDF generated for noids-report-2015-week-22.csv\n",
            "PDF generated for noids-week-41-csv.csv\n",
            "PDF generated for noids-report-2015-week-9.csv\n",
            "PDF generated for noids-report-2021-weeks-32-35.csv\n",
            "PDF generated for noids-week-24-csv.csv\n",
            "PDF generated for noids-report-2015-week-28.csv\n",
            "PDF generated for noids-week-1-3-2021.csv\n",
            "PDF generated for noids-report-2015-week-31.csv\n",
            "PDF generated for noids-report-2015-week-36.csv\n",
            "PDF generated for copy-of-noids-week-48-csv.csv\n",
            "PDF generated for noids-report-2023-week-17-20.csv\n",
            "PDF generated for noids-report-2021-week-52.csv\n",
            "PDF generated for noids-report-week-37--40-2022.csv\n",
            "PDF generated for noids-report-2015-week-38.csv\n",
            "PDF generated for noidss-report-2015-week-21.csv\n",
            "PDF generated for noids-week-51-to-week-53-2020.csv\n",
            "PDF generated for noids-report-2015-week-23.csv\n",
            "PDF generated for noids-report-week--29---32--2022.csv\n",
            "PDF generated for noids-week-44-to-week-46-2020.csv\n",
            "PDF generated for noids-report-2015-week-6.csv\n",
            "PDF generated for noids-report-week-44-2022-csv.csv\n",
            "PDF generated for weekly-noids-report-week-16-2015.csv\n",
            "PDF generated for noids-report-2023-week-13-16.csv\n",
            "PDF generated for copy-of-noids-week-52-csv.csv\n",
            "PDF generated for weekly-noids-report-2015-week-14.csv\n",
            "PDF generated for noids-week-23--26-2021.csv\n",
            "PDF generated for noids-week-49-csv.csv\n",
            "PDF generated for weekly-noids-report-11.csv\n",
            "PDF generated for noids-week-33-csv-.csv\n",
            "PDF generated for noids-report-2015-week-40.csv\n",
            "PDF generated for noids-report-2015-week-7.csv\n",
            "PDF generated for noids-report-week-20-2015.csv\n",
            "PDF generated for noids-week-32-csv-.csv\n",
            "PDF generated for noids-report--week-21---24.csv\n",
            "PDF generated for weekly-noids-report-week-18-2015.csv\n",
            "PDF generated for noids-report-2015-week-32.csv\n",
            "PDF generated for noids-report-2015-week-29.csv\n",
            "PDF generated for noids-week-15-18-2021.csv\n",
            "PDF generated for noids-report-2015-week-30.csv\n",
            "PDF generated for noids-week-36-39-2021.csv\n",
            "PDF generated for weekly-noids-report-13-2015.csv\n",
            "PDF generated for noids--report-week--9---12--2022.csv\n",
            "PDF generated for noids-report-2021-weeks-44-47.csv\n",
            "PDF generated for noids-report-2021-week-40-43.csv\n",
            "PDF generated for noids-report-2015-week-37.csv\n",
            "PDF generated for weekly-noids-report-2015-week-12.csv\n",
            "PDF generated for noids-report-2015-week-39.csv\n",
            "PDF generated for noids-report-2015-week-27.csv\n",
            "PDF generated for noids-report-week-13---16--2022.csv\n",
            "PDF generated for noids-week-11--14-2021-.csv\n",
            "PDF generated for noids--report-2021-weeks-48--51.csv\n",
            "PDF generated for noids-week-40---week-43.csv\n",
            "PDF generated for weekly-noids-report-week-19-2015.csv\n",
            "PDF generated for noids-week-52-csv.csv\n",
            "PDF generated for noids-week-37-csv.csv\n",
            "PDF generated for noids-report-week--5---8-2022.csv\n",
            "PDF generated for noids-report-week-17---20--2022.csv\n",
            "PDF generated for weekly-noids-report-week-15-2015.csv\n",
            "PDF generated for noids--report-week-1---4--2022.csv\n",
            "PDF generated for noids-week-30-and-31-2021.csv\n",
            "PDF generated for noids-report-2015-week-24.csv\n",
            "PDF generated for noids-week-19---22-2021.csv\n",
            "PDF generated for noids-report-2015-week-33.csv\n",
            "PDF generated for noids-week-7-10-2021-updated.csv\n",
            "PDF generated for noids-week-47-to-week-50-2020.csv\n",
            "PDF generated for noids-report-2015-week-26.csv\n",
            "PDF generated for noids-report-week-10-2015.csv\n",
            "PDF generated for noids-report-2015-week-35.csv\n",
            "PDF generated for noids-report-week-33---36-2022.csv\n",
            "PDF generated for noids-week-36-to-week-39.csv\n",
            "PDF generated for noids-week-1-4-2024-.csv\n",
            "PDF generated for noids-report-week-25---28--2022.csv\n",
            "PDF generated for noids-report-week-17-2015.csv\n",
            "PDF generated for noids-report-2015-week-25.csv\n",
            "PDF generated for noids-report-2015-week-53.csv\n",
            "PDF generated for noids-week-45-csv.csv\n",
            "PDF generated for noids-week-27---29-2021.csv\n",
            "PDF generated for noids-report-2015-week-8.csv\n",
            "PDF generated for noids-report-2015-week-34.csv\n",
            "PDF generated for noids-week-28-csv.csv\n",
            "All PDFs have been merged into: /content/pdfs/merged_output.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Define the path to your CSV files\n",
        "csv_directory = '/content/csvs'\n",
        "\n",
        "# Load all CSV files in the directory into DataFrames\n",
        "csv_files = glob(os.path.join(csv_directory, '*.csv'))\n",
        "dataframes = []\n",
        "\n",
        "for file in csv_files:\n",
        "    # Correct header processing, assuming the second row is the actual header\n",
        "    df = pd.read_csv(file, header=1)\n",
        "    # Convert week data columns to numeric, errors='coerce' will handle non-numeric issues\n",
        "    for col in df.columns[1:]:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into one DataFrame\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Clean data, such as removing rows where 'Disease' is NaN\n",
        "combined_df.dropna(subset=['Disease'], inplace=True)\n",
        "\n",
        "# Aggregate data, here summing up all years and weeks per disease\n",
        "aggregated_df = combined_df.groupby('Disease').sum()\n",
        "\n",
        "# Export the aggregated data to a new CSV\n",
        "output_file = os.path.join(csv_directory, 'aggregated_output.csv')\n",
        "aggregated_df.to_csv(output_file, index=True)\n",
        "\n",
        "print(f\"Aggregated data has been saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnnvuaSRde4Q",
        "outputId": "323a54a2-0bd2-4285-adfc-b02cc7be05d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data has been saved to /content/csvs/aggregated_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import re\n",
        "\n",
        "def get_year_from_column_name(column_name):\n",
        "    # Extract the year from the column name using regex\n",
        "    match = re.search(r'20\\d{2}', column_name)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "# Define the path to your CSV files\n",
        "csv_directory = '/content/csvs'\n",
        "\n",
        "# List of all CSV files in the directory\n",
        "csv_files = glob(os.path.join(csv_directory, '*.csv'))\n",
        "all_yearly_data = {}\n",
        "\n",
        "for file in csv_files:\n",
        "    # Determine the header row by reading the file line by line until we find 'Disease'\n",
        "    header_row = None\n",
        "    with open(file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if 'Disease' in line:\n",
        "                header_row = i\n",
        "                break\n",
        "\n",
        "    # If the header row is found, read the CSV file from that row\n",
        "    if header_row is not None:\n",
        "        df = pd.read_csv(file, header=header_row)\n",
        "\n",
        "    # Extract year from the last three column names and rename the columns to just the year\n",
        "    last_three_cols = df.columns[-3:]\n",
        "    for col in last_three_cols:\n",
        "        year = get_year_from_column_name(col)\n",
        "        if year:\n",
        "            # Rename the column to be just the year\n",
        "            df.rename(columns={col: year}, inplace=True)\n",
        "            # Ensure the column is numeric\n",
        "            df[year] = pd.to_numeric(df[year], errors='coerce')\n",
        "\n",
        "            # Aggregate the data\n",
        "            if year in all_yearly_data:\n",
        "                # If the year is already in the dict, we check the total sum\n",
        "                current_year_sum = df[year].sum()\n",
        "                if current_year_sum > all_yearly_data[year]['sum']:\n",
        "                    all_yearly_data[year]['data'] = df[['Disease', year]]\n",
        "                    all_yearly_data[year]['sum'] = current_year_sum\n",
        "            else:\n",
        "                all_yearly_data[year] = {'data': df[['Disease', year]], 'sum': df[year].sum()}\n",
        "\n",
        "# Combine all the dataframes with yearly data into one dataframe\n",
        "final_df = pd.DataFrame()\n",
        "for year_info in all_yearly_data.values():\n",
        "    yearly_df = year_info['data']\n",
        "    if final_df.empty:\n",
        "        final_df = yearly_df\n",
        "    else:\n",
        "        final_df = final_df.merge(yearly_df, on='Disease', how='outer')\n",
        "\n",
        "# Replace NaN with zeros and set 'Disease' as the first column (index reset)\n",
        "final_df.fillna(0, inplace=True)\n",
        "final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save the final DataFrame to a CSV file\n",
        "output_file = os.path.join(csv_directory, 'aggregated_yearly_data.csv')\n",
        "final_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Aggregated data has been saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRMEBLHndp6-",
        "outputId": "674f7797-6d50-4f3c-ad5e-965c3b5b7795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data has been saved to /content/csvs/aggregated_yearly_data.csv\n"
          ]
        }
      ]
    }
  ]
}